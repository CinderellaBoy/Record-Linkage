{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jellyfish\n",
    "from unidecode import unidecode\n",
    "from rapidfuzz import fuzz, distance\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8798c474",
   "metadata": {},
   "source": [
    "### 2. 数据预处理\n",
    "\n",
    "在这一部分，我们对数据进行标准化处理：\n",
    "- 将所有的名字转为小写字母\n",
    "- 去除名字中的音符符号（如重音）\n",
    "- 清理特殊字符，只保留字母、数字和空格\n",
    "- 合并多余的空格\n",
    "\n",
    "这一步确保我们对数据进行统一标准化，以便后续匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a5b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理名称的标准化方法\n",
    "def normalize(name: str) -> str:\n",
    "    # 去除音符符号\n",
    "    name = unidecode(name)\n",
    "    # 转为小写\n",
    "    name = name.lower()\n",
    "    # 清理特殊字符（除字母、数字、空格外的所有字符）\n",
    "    name = re.sub(r\"[^a-z0-9\\s]\", \" \", name)\n",
    "    # 合并多余的空格\n",
    "    name = \" \".join(name.split())\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7033699",
   "metadata": {},
   "source": [
    "### 3. LSH (MinHash) 特征计算\n",
    "\n",
    "通过计算每个名称的 MinHash 特征来准备阻塞步骤。MinHash 是一种用于**局部敏感哈希 (LSH)** 的技术，可以通过比较小的指纹来加速相似性比较。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af8c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSH (MinHash) 特征计算\n",
    "def minhash_features(name):\n",
    "    m = MinHash()\n",
    "    for d in name.split():\n",
    "        m.update(d.encode('utf8'))\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242befb",
   "metadata": {},
   "source": [
    "### 4. 阻塞技术：使用 LSH 或 Soundex\n",
    "\n",
    "我们在这里实现了两种阻塞方法：\n",
    "1. **LSH**（局部敏感哈希）：我们通过 MinHash 计算每个名称的指纹，并使用 LSH 来加速记录匹配。\n",
    "2. **Soundex**：基于名称的 Soundex 码进行阻塞，适用于拼写变体较小的情况。\n",
    "\n",
    "可以通过设置 `use_lsh` 为 `True` 或 `False` 来选择不同的阻塞方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 阻塞操作 - 使用LSH\n",
    "def generate_block_keys(df, use_lsh=False):\n",
    "    if use_lsh:\n",
    "        df['minhash'] = df['norm_name'].apply(minhash_features)\n",
    "        return df\n",
    "    else:\n",
    "        df[\"soundex_key\"] = df[\"norm_name\"].apply(lambda x: jellyfish.soundex(x))\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687b234",
   "metadata": {},
   "source": [
    "### 5. LSH 阻塞实现\n",
    "\n",
    "我们通过 LSH 将相似的记录分到相同的块中。这样，我们只需比较属于同一块的记录，避免了计算每个记录对的所有可能组合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSH 阻塞\n",
    "def lsh_blocking(df, threshold=0.8):\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
    "    candidates = []\n",
    "    for idx, row in df.iterrows():\n",
    "        lsh.insert(row['ID'], row['minhash'])\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        result = lsh.query(row['minhash'])\n",
    "        for match_id in result:\n",
    "            if match_id != row['ID']:\n",
    "                candidates.append((row['ID'], match_id))\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043463a4",
   "metadata": {},
   "source": [
    "### 6. 提取候选对的相似度特征\n",
    "\n",
    "我们使用多种方法计算记录对之间的相似度，如 **Jaro-Winkler 相似度**、**Levenshtein 距离** 和 **Token Set Ratio**。这些特征将在后续的分类模型中作为输入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9997d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取相似度特征\n",
    "def build_features(a, b):\n",
    "    return {\n",
    "        \"jw\": fuzz.WRatio(a, b, score_cutoff=0)/100,  # Jaro-Winkler 相似度\n",
    "        \"lev_ratio\": 1 - distance.Levenshtein.normalized_distance(a, b),  # Levenshtein 相似度\n",
    "        \"token_set\": fuzz.token_set_ratio(a, b)/100,  # Token Set Ratio\n",
    "        \"prefix_match\": int(a[:4] == b[:4]),  # 前缀匹配\n",
    "        \"len_diff\": abs(len(a) - len(b)),  # 长度差异\n",
    "        \"same_soundex\": int(jellyfish.soundex(a) == jellyfish.soundex(b))  # Soundex匹配\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db0fe2",
   "metadata": {},
   "source": [
    "### 7. 抽取候选对\n",
    "\n",
    "我们从 `primary_df` 和 `alternate_df` 中抽取候选对。如果两个记录属于同一块（通过阻塞键判断），我们将计算它们之间的相似度特征。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c67914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽取候选对：同block的记录进行匹配\n",
    "def extract_candidates(primary_df, alternate_df, use_lsh=False):\n",
    "    candidates = []\n",
    "    for _, primary_row in primary_df.iterrows():\n",
    "        for _, alternate_row in alternate_df.iterrows():\n",
    "            if (use_lsh and primary_row['minhash'] == alternate_row['minhash']) or \\\n",
    "               (not use_lsh and primary_row['soundex_key'] == alternate_row['soundex_key']):\n",
    "                features = build_features(primary_row['norm_name'], alternate_row['norm_name'])\n",
    "                candidates.append({\n",
    "                    'id_left': primary_row['ID'],\n",
    "                    'id_right': alternate_row['ID'],\n",
    "                    'name_left': primary_row['NAME'],\n",
    "                    'name_right': alternate_row['NAME'],\n",
    "                    'Y/N': 'Y' if primary_row['ID'] == alternate_row['ID'] else 'N',\n",
    "                    **features\n",
    "                })\n",
    "    return pd.DataFrame(candidates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0281b0c",
   "metadata": {},
   "source": [
    "### 8. 读取数据\n",
    "\n",
    "在这一部分，我们加载了 `primary.csv` 和 `alternate.csv` 数据，并进行标准化处理。同时，我们加载 `test_01.csv` 数据，准备进行映射任务。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a768547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "primary_df = pd.read_csv('/mnt/data/primary.csv')\n",
    "alternate_df = pd.read_csv('/mnt/data/alternate.csv')\n",
    "test_df = pd.read_csv('/mnt/data/test_01.csv')\n",
    "\n",
    "# 标准化数据\n",
    "primary_df['norm_name'] = primary_df['NAME'].apply(normalize)\n",
    "alternate_df['norm_name'] = alternate_df['NAME'].apply(normalize)\n",
    "\n",
    "# 生成阻塞键\n",
    "use_lsh = True  # 使用 LSH 阻塞\n",
    "primary_df = generate_block_keys(primary_df, use_lsh=use_lsh)\n",
    "alternate_df = generate_block_keys(alternate_df, use_lsh=use_lsh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20b2a2",
   "metadata": {},
   "source": [
    "### 9. 训练模型：随机森林和朴素贝叶斯\n",
    "\n",
    "我们使用 **随机森林（RandomForestClassifier）** 和 **朴素贝叶斯（Naive Bayes）** 来对记录匹配进行分类。我们评估了两个模型的 **Precision**、**Recall** 和 **F1-Score**，以比较它们的表现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e14813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林模型\n",
    "rf_model = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 评估：使用5折交叉验证\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_score(rf_model, X, y, cv=cv, scoring='accuracy').mean()\n",
    "\n",
    "# 计算模型在测试集上的表现\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "precision_rf, recall_rf, f1_rf, _ = precision_recall_fscore_support(y_test, y_pred_rf, average='binary')\n",
    "\n",
    "print(f\"Random Forest Precision: {precision_rf:.3f}, Recall: {recall_rf:.3f}, F1-Score: {f1_rf:.3f}\")\n",
    "\n",
    "# 朴素贝叶斯模型\n",
    "nb_model = BernoulliNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# 评估贝叶斯模型\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "precision_nb, recall_nb, f1_nb, _ = precision_recall_fscore_support(y_test, y_pred_nb, average='binary')\n",
    "\n",
    "print(f\"Naive Bayes Precision: {precision_nb:.3f}, Recall: {recall_nb:.3f}, F1-Score: {f1_nb:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e8a4bc",
   "metadata": {},
   "source": [
    "### 10. 预测结果输出\n",
    "\n",
    "在此部分，我们输出了每个测试样本的预测结果以及相应的概率。最终结果被保存到 `predictions_with_proba.csv` 文件中，供后续分析使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47cd9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出模型概率预测 (proba)\n",
    "rf_probs = rf_model.predict_proba(X_test)[:, 1]  # 获取 positive 类别的概率\n",
    "nb_probs = nb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 输出结果 (Y/N) 及对应的概率\n",
    "output_df = X_test.copy()\n",
    "output_df['rf_y_pred'] = rf_model.predict(X_test)\n",
    "output_df['rf_y_proba'] = rf_probs\n",
    "output_df['nb_y_pred'] = nb_model.predict(X_test)\n",
    "output_df['nb_y_proba'] = nb_probs\n",
    "\n",
    "# 合并原始名称数据\n",
    "output_df['id_left'] = y_test.index\n",
    "output_df['id_right'] = X_test.index\n",
    "\n",
    "# 最终结果导出\n",
    "output_df.to_csv('predictions_with_proba.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
